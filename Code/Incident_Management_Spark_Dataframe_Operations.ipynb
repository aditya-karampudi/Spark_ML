{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de49a34d-9611-4e97-a5a2-df407a421ef7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["spark = SparkSession.builder.appName('IMDF').getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"44fd60b0-ad90-4159-8734-81bb4297521e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df = spark.read.csv('dbfs:/FileStore/tables/incident_event_log_reduced.csv',inferSchema=True,header=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3456db6c-2c30-47bd-b7e7-a1376848fbd4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"number","nullable":true,"type":"string"},{"metadata":{},"name":"incident_state","nullable":true,"type":"string"},{"metadata":{},"name":"active","nullable":true,"type":"boolean"},{"metadata":{},"name":"reassignment_count","nullable":true,"type":"integer"},{"metadata":{},"name":"reopen_count","nullable":true,"type":"integer"},{"metadata":{},"name":"sys_mod_count","nullable":true,"type":"integer"},{"metadata":{},"name":"made_sla","nullable":true,"type":"boolean"},{"metadata":{},"name":"caller_id","nullable":true,"type":"string"},{"metadata":{},"name":"opened_by","nullable":true,"type":"string"},{"metadata":{},"name":"opened_at","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_at","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_at","nullable":true,"type":"string"},{"metadata":{},"name":"contact_type","nullable":true,"type":"string"},{"metadata":{},"name":"location","nullable":true,"type":"string"},{"metadata":{},"name":"category","nullable":true,"type":"string"},{"metadata":{},"name":"subcategory","nullable":true,"type":"string"},{"metadata":{},"name":"u_symptom","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci","nullable":true,"type":"string"},{"metadata":{},"name":"impact","nullable":true,"type":"string"},{"metadata":{},"name":"urgency","nullable":true,"type":"string"},{"metadata":{},"name":"priority","nullable":true,"type":"string"},{"metadata":{},"name":"assignment_group","nullable":true,"type":"string"},{"metadata":{},"name":"assigned_to","nullable":true,"type":"string"},{"metadata":{},"name":"knowledge","nullable":true,"type":"boolean"},{"metadata":{},"name":"u_priority_confirmation","nullable":true,"type":"boolean"},{"metadata":{},"name":"notify","nullable":true,"type":"string"},{"metadata":{},"name":"problem_id","nullable":true,"type":"string"},{"metadata":{},"name":"rfc","nullable":true,"type":"string"},{"metadata":{},"name":"vendor","nullable":true,"type":"string"},{"metadata":{},"name":"caused_by","nullable":true,"type":"string"},{"metadata":{},"name":"closed_code","nullable":true,"type":"string"},{"metadata":{},"name":"resolved_by","nullable":true,"type":"string"},{"metadata":{},"name":"resolved_at","nullable":true,"type":"string"},{"metadata":{},"name":"closed_at","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f779606-4829-435c-a514-a1fc9fd17f23"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">root\n |-- number: string (nullable = true)\n |-- incident_state: string (nullable = true)\n |-- active: boolean (nullable = true)\n |-- reassignment_count: integer (nullable = true)\n |-- reopen_count: integer (nullable = true)\n |-- sys_mod_count: integer (nullable = true)\n |-- made_sla: boolean (nullable = true)\n |-- caller_id: string (nullable = true)\n |-- opened_by: string (nullable = true)\n |-- opened_at: string (nullable = true)\n |-- sys_created_by: string (nullable = true)\n |-- sys_created_at: string (nullable = true)\n |-- sys_updated_by: string (nullable = true)\n |-- sys_updated_at: string (nullable = true)\n |-- contact_type: string (nullable = true)\n |-- location: string (nullable = true)\n |-- category: string (nullable = true)\n |-- subcategory: string (nullable = true)\n |-- u_symptom: string (nullable = true)\n |-- cmdb_ci: string (nullable = true)\n |-- impact: string (nullable = true)\n |-- urgency: string (nullable = true)\n |-- priority: string (nullable = true)\n |-- assignment_group: string (nullable = true)\n |-- assigned_to: string (nullable = true)\n |-- knowledge: boolean (nullable = true)\n |-- u_priority_confirmation: boolean (nullable = true)\n |-- notify: string (nullable = true)\n |-- problem_id: string (nullable = true)\n |-- rfc: string (nullable = true)\n |-- vendor: string (nullable = true)\n |-- caused_by: string (nullable = true)\n |-- closed_code: string (nullable = true)\n |-- resolved_by: string (nullable = true)\n |-- resolved_at: string (nullable = true)\n |-- closed_at: string (nullable = true)\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- number: string (nullable = true)\n-- incident_state: string (nullable = true)\n-- active: boolean (nullable = true)\n-- reassignment_count: integer (nullable = true)\n-- reopen_count: integer (nullable = true)\n-- sys_mod_count: integer (nullable = true)\n-- made_sla: boolean (nullable = true)\n-- caller_id: string (nullable = true)\n-- opened_by: string (nullable = true)\n-- opened_at: string (nullable = true)\n-- sys_created_by: string (nullable = true)\n-- sys_created_at: string (nullable = true)\n-- sys_updated_by: string (nullable = true)\n-- sys_updated_at: string (nullable = true)\n-- contact_type: string (nullable = true)\n-- location: string (nullable = true)\n-- category: string (nullable = true)\n-- subcategory: string (nullable = true)\n-- u_symptom: string (nullable = true)\n-- cmdb_ci: string (nullable = true)\n-- impact: string (nullable = true)\n-- urgency: string (nullable = true)\n-- priority: string (nullable = true)\n-- assignment_group: string (nullable = true)\n-- assigned_to: string (nullable = true)\n-- knowledge: boolean (nullable = true)\n-- u_priority_confirmation: boolean (nullable = true)\n-- notify: string (nullable = true)\n-- problem_id: string (nullable = true)\n-- rfc: string (nullable = true)\n-- vendor: string (nullable = true)\n-- caused_by: string (nullable = true)\n-- closed_code: string (nullable = true)\n-- resolved_by: string (nullable = true)\n-- resolved_at: string (nullable = true)\n-- closed_at: string (nullable = true)\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import datediff,date_format,to_date,to_timestamp"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12e7954d-44c9-4486-96e4-4095ba4e2bfd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df=df.withColumn('resolved_ts',to_timestamp(df.resolved_at, 'dd/MM/yyyy HH:mm')).\\\n                withColumn('opened_ts',to_timestamp(df.opened_at, 'dd/MM/yyyy HH:mm')).\\\n                withColumn('sys_created_ts',to_timestamp(df.sys_created_at, 'dd/MM/yyyy HH:mm')).\\\n                withColumn('sys_updated_ts',to_timestamp(df.sys_updated_at, 'dd/MM/yyyy HH:mm')).\\\n                withColumn('closed_ts',to_timestamp(df.closed_at, 'dd/MM/yyyy HH:mm')).\\\n                withColumn('resolved',to_date(df.resolved_at, 'dd/MM/yyyy HH:mm')).\\\n                withColumn('opened',to_date(df.opened_at, 'dd/MM/yyyy HH:mm')).\\\n                withColumn('sys_created',to_date(df.sys_created_at, 'dd/MM/yyyy HH:mm')).\\\n                withColumn('sys_updated',to_date(df.sys_updated_at, 'dd/MM/yyyy HH:mm')).\\\n                withColumn('closed',to_date(df.closed_at, 'dd/MM/yyyy HH:mm')).\\\n                withColumn('duration',datediff(to_date(df.resolved_at, 'dd/MM/yyyy HH:mm'),to_date(df.opened_at, 'dd/MM/yyyy HH:mm')))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98a5105b-7f39-4e56-b0ab-b96ab6ba754c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"number","nullable":true,"type":"string"},{"metadata":{},"name":"incident_state","nullable":true,"type":"string"},{"metadata":{},"name":"active","nullable":true,"type":"boolean"},{"metadata":{},"name":"reassignment_count","nullable":true,"type":"integer"},{"metadata":{},"name":"reopen_count","nullable":true,"type":"integer"},{"metadata":{},"name":"sys_mod_count","nullable":true,"type":"integer"},{"metadata":{},"name":"made_sla","nullable":true,"type":"boolean"},{"metadata":{},"name":"caller_id","nullable":true,"type":"string"},{"metadata":{},"name":"opened_by","nullable":true,"type":"string"},{"metadata":{},"name":"opened_at","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_at","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_at","nullable":true,"type":"string"},{"metadata":{},"name":"contact_type","nullable":true,"type":"string"},{"metadata":{},"name":"location","nullable":true,"type":"string"},{"metadata":{},"name":"category","nullable":true,"type":"string"},{"metadata":{},"name":"subcategory","nullable":true,"type":"string"},{"metadata":{},"name":"u_symptom","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci","nullable":true,"type":"string"},{"metadata":{},"name":"impact","nullable":true,"type":"string"},{"metadata":{},"name":"urgency","nullable":true,"type":"string"},{"metadata":{},"name":"priority","nullable":true,"type":"string"},{"metadata":{},"name":"assignment_group","nullable":true,"type":"string"},{"metadata":{},"name":"assigned_to","nullable":true,"type":"string"},{"metadata":{},"name":"knowledge","nullable":true,"type":"boolean"},{"metadata":{},"name":"u_priority_confirmation","nullable":true,"type":"boolean"},{"metadata":{},"name":"notify","nullable":true,"type":"string"},{"metadata":{},"name":"problem_id","nullable":true,"type":"string"},{"metadata":{},"name":"rfc","nullable":true,"type":"string"},{"metadata":{},"name":"vendor","nullable":true,"type":"string"},{"metadata":{},"name":"caused_by","nullable":true,"type":"string"},{"metadata":{},"name":"closed_code","nullable":true,"type":"string"},{"metadata":{},"name":"resolved_by","nullable":true,"type":"string"},{"metadata":{},"name":"resolved_at","nullable":true,"type":"string"},{"metadata":{},"name":"closed_at","nullable":true,"type":"string"},{"metadata":{},"name":"resolved_ts","nullable":true,"type":"timestamp"},{"metadata":{},"name":"opened_ts","nullable":true,"type":"timestamp"},{"metadata":{},"name":"sys_created_ts","nullable":true,"type":"timestamp"},{"metadata":{},"name":"sys_updated_ts","nullable":true,"type":"timestamp"},{"metadata":{},"name":"closed_ts","nullable":true,"type":"timestamp"},{"metadata":{},"name":"resolved","nullable":true,"type":"date"},{"metadata":{},"name":"opened","nullable":true,"type":"date"},{"metadata":{},"name":"sys_created","nullable":true,"type":"date"},{"metadata":{},"name":"sys_updated","nullable":true,"type":"date"},{"metadata":{},"name":"closed","nullable":true,"type":"date"},{"metadata":{},"name":"duration","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_unique_incidents=df.filter(\"incident_state=='Closed'\").sort(\"sys_mod_count\",ascending=False).dropDuplicates([\"number\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35fc6c1e-de24-4e60-b3ae-dfeaa29e0a4d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"df_unique_incidents","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"number","nullable":true,"type":"string"},{"metadata":{},"name":"incident_state","nullable":true,"type":"string"},{"metadata":{},"name":"active","nullable":true,"type":"boolean"},{"metadata":{},"name":"reassignment_count","nullable":true,"type":"integer"},{"metadata":{},"name":"reopen_count","nullable":true,"type":"integer"},{"metadata":{},"name":"sys_mod_count","nullable":true,"type":"integer"},{"metadata":{},"name":"made_sla","nullable":true,"type":"boolean"},{"metadata":{},"name":"caller_id","nullable":true,"type":"string"},{"metadata":{},"name":"opened_by","nullable":true,"type":"string"},{"metadata":{},"name":"opened_at","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_created_at","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_by","nullable":true,"type":"string"},{"metadata":{},"name":"sys_updated_at","nullable":true,"type":"string"},{"metadata":{},"name":"contact_type","nullable":true,"type":"string"},{"metadata":{},"name":"location","nullable":true,"type":"string"},{"metadata":{},"name":"category","nullable":true,"type":"string"},{"metadata":{},"name":"subcategory","nullable":true,"type":"string"},{"metadata":{},"name":"u_symptom","nullable":true,"type":"string"},{"metadata":{},"name":"cmdb_ci","nullable":true,"type":"string"},{"metadata":{},"name":"impact","nullable":true,"type":"string"},{"metadata":{},"name":"urgency","nullable":true,"type":"string"},{"metadata":{},"name":"priority","nullable":true,"type":"string"},{"metadata":{},"name":"assignment_group","nullable":true,"type":"string"},{"metadata":{},"name":"assigned_to","nullable":true,"type":"string"},{"metadata":{},"name":"knowledge","nullable":true,"type":"boolean"},{"metadata":{},"name":"u_priority_confirmation","nullable":true,"type":"boolean"},{"metadata":{},"name":"notify","nullable":true,"type":"string"},{"metadata":{},"name":"problem_id","nullable":true,"type":"string"},{"metadata":{},"name":"rfc","nullable":true,"type":"string"},{"metadata":{},"name":"vendor","nullable":true,"type":"string"},{"metadata":{},"name":"caused_by","nullable":true,"type":"string"},{"metadata":{},"name":"closed_code","nullable":true,"type":"string"},{"metadata":{},"name":"resolved_by","nullable":true,"type":"string"},{"metadata":{},"name":"resolved_at","nullable":true,"type":"string"},{"metadata":{},"name":"closed_at","nullable":true,"type":"string"},{"metadata":{},"name":"resolved_ts","nullable":true,"type":"timestamp"},{"metadata":{},"name":"opened_ts","nullable":true,"type":"timestamp"},{"metadata":{},"name":"sys_created_ts","nullable":true,"type":"timestamp"},{"metadata":{},"name":"sys_updated_ts","nullable":true,"type":"timestamp"},{"metadata":{},"name":"closed_ts","nullable":true,"type":"timestamp"},{"metadata":{},"name":"resolved","nullable":true,"type":"date"},{"metadata":{},"name":"opened","nullable":true,"type":"date"},{"metadata":{},"name":"sys_created","nullable":true,"type":"date"},{"metadata":{},"name":"sys_updated","nullable":true,"type":"date"},{"metadata":{},"name":"closed","nullable":true,"type":"date"},{"metadata":{},"name":"duration","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 1. Top 5 people with most resolved incidents"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8fb03cc8-0bfd-486d-88ad-33a9a1c95a2c"}}},{"cell_type":"code","source":["A1=df_unique_incidents.groupby(\"resolved_by\").count().sort(\"count\",ascending=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef7cf495-9358-48f5-9350-88a13175259c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"A1","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"resolved_by","nullable":true,"type":"string"},{"metadata":{},"name":"count","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["A1.show(n=5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e79de3d4-8516-4a5e-bf59-a999ecf45d4d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+---------------+-----+\n|    resolved_by|count|\n+---------------+-----+\n| Resolved by 11| 3071|\n| Resolved by 15| 2415|\n|Resolved by 103|  689|\n|Resolved by 177|  686|\n| Resolved by 32|  597|\n+---------------+-----+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+-----+\n    resolved_by|count|\n+---------------+-----+\n Resolved by 11| 3071|\n Resolved by 15| 2415|\nResolved by 103|  689|\nResolved by 177|  686|\n Resolved by 32|  597|\n+---------------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 2. Based on least average duration, find the top 5 people with maxmium number of incidents resolved"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52f07f14-88cb-4cbd-9b6b-42e83a3fe163"}}},{"cell_type":"code","source":["from pyspark.sql import functions as F"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb1262b5-c06d-43b3-9c18-7518f17bd911"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["A2= df_unique_incidents.groupby(\"resolved_by\").agg(F.count(\"duration\"),F.mean(\"duration\")).\\\n    withColumnRenamed(\"count(duration)\",\"Incidents Resolved\").\\\n    withColumnRenamed(\"avg(duration)\",\"Average Duration\").\\\n    orderBy([\"Average Duration\",\"Incidents Resolved\"],ascending=[True,False])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b470c6fe-849e-4ead-9b8a-f213bc93290b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"A2","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"resolved_by","nullable":true,"type":"string"},{"metadata":{},"name":"Incidents Resolved","nullable":false,"type":"long"},{"metadata":{},"name":"Average Duration","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["A2.show(n=5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a10ceccc-4dec-4fc3-83ae-0cd58f73cbe7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-699501117654355&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>A2<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">5</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    439</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    440</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 441</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    442</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    443</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1092.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 140.0 failed 1 times, most recent failure: Lost task 0.0 in stage 140.0 (TID 97, ip-10-172-253-6.us-west-2.compute.internal, executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;29/2/2016 11:29&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:150)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:86)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:733)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:896)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:896)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:320)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.time.format.DateTimeParseException: Text &#39;29/2/2016 11:29&#39; could not be parsed at index 3\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2352)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2371)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2396)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1010)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\n\tat org.apache.spark.RangePartitioner.&lt;init&gt;(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:261)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:143)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:117)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:113)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:192)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:192)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$3(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:87)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4(AdaptiveSparkPlanExec.scala:217)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4$adapted(AdaptiveSparkPlanExec.scala:215)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:215)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:836)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:201)\n\tat org.apache.spark.sql.execution.collect.Collector$.callExecuteCollect(Collector.scala:117)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:69)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:480)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:396)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:2987)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3710)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2711)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3702)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:249)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:836)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:199)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3700)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2711)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2918)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:305)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:342)\n\tat sun.reflect.GeneratedMethodAccessor549.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;29/2/2016 11:29&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:150)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:86)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:733)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:896)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:896)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:320)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text &#39;29/2/2016 11:29&#39; could not be parsed at index 3\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n\t... 25 more\n</div>","errorSummary":"org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 140.0 failed 1 times, most recent failure: Lost task 0.0 in stage 140.0 (TID 97, ip-10-172-253-6.us-west-2.compute.internal, executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;29/2/2016 11:29&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-699501117654355&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>A2<span class=\"ansi-blue-fg\">.</span>show<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">=</span><span class=\"ansi-cyan-fg\">5</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansi-cyan-fg\">show</span><span class=\"ansi-blue-fg\">(self, n, truncate, vertical)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    439</span>         &#34;&#34;&#34;\n<span class=\"ansi-green-intense-fg ansi-bold\">    440</span>         <span class=\"ansi-green-fg\">if</span> isinstance<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">,</span> bool<span class=\"ansi-blue-fg\">)</span> <span class=\"ansi-green-fg\">and</span> truncate<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 441</span><span class=\"ansi-red-fg\">             </span>print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-cyan-fg\">20</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    442</span>         <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    443</span>             print<span class=\"ansi-blue-fg\">(</span>self<span class=\"ansi-blue-fg\">.</span>_jdf<span class=\"ansi-blue-fg\">.</span>showString<span class=\"ansi-blue-fg\">(</span>n<span class=\"ansi-blue-fg\">,</span> int<span class=\"ansi-blue-fg\">(</span>truncate<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">,</span> vertical<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py</span> in <span class=\"ansi-cyan-fg\">__call__</span><span class=\"ansi-blue-fg\">(self, *args)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1303</span>         answer <span class=\"ansi-blue-fg\">=</span> self<span class=\"ansi-blue-fg\">.</span>gateway_client<span class=\"ansi-blue-fg\">.</span>send_command<span class=\"ansi-blue-fg\">(</span>command<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">   1304</span>         return_value = get_return_value(\n<span class=\"ansi-green-fg\">-&gt; 1305</span><span class=\"ansi-red-fg\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">   1306</span> \n<span class=\"ansi-green-intense-fg ansi-bold\">   1307</span>         <span class=\"ansi-green-fg\">for</span> temp_arg <span class=\"ansi-green-fg\">in</span> temp_args<span class=\"ansi-blue-fg\">:</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansi-cyan-fg\">deco</span><span class=\"ansi-blue-fg\">(*a, **kw)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    125</span>     <span class=\"ansi-green-fg\">def</span> deco<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    126</span>         <span class=\"ansi-green-fg\">try</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-fg\">--&gt; 127</span><span class=\"ansi-red-fg\">             </span><span class=\"ansi-green-fg\">return</span> f<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">*</span>a<span class=\"ansi-blue-fg\">,</span> <span class=\"ansi-blue-fg\">**</span>kw<span class=\"ansi-blue-fg\">)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    128</span>         <span class=\"ansi-green-fg\">except</span> py4j<span class=\"ansi-blue-fg\">.</span>protocol<span class=\"ansi-blue-fg\">.</span>Py4JJavaError <span class=\"ansi-green-fg\">as</span> e<span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    129</span>             converted <span class=\"ansi-blue-fg\">=</span> convert_exception<span class=\"ansi-blue-fg\">(</span>e<span class=\"ansi-blue-fg\">.</span>java_exception<span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-green-fg\">/databricks/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py</span> in <span class=\"ansi-cyan-fg\">get_return_value</span><span class=\"ansi-blue-fg\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    326</span>                 raise Py4JJavaError(\n<span class=\"ansi-green-intense-fg ansi-bold\">    327</span>                     <span class=\"ansi-blue-fg\">&#34;An error occurred while calling {0}{1}{2}.\\n&#34;</span><span class=\"ansi-blue-fg\">.</span>\n<span class=\"ansi-green-fg\">--&gt; 328</span><span class=\"ansi-red-fg\">                     format(target_id, &#34;.&#34;, name), value)\n</span><span class=\"ansi-green-intense-fg ansi-bold\">    329</span>             <span class=\"ansi-green-fg\">else</span><span class=\"ansi-blue-fg\">:</span>\n<span class=\"ansi-green-intense-fg ansi-bold\">    330</span>                 raise Py4JError(\n\n<span class=\"ansi-red-fg\">Py4JJavaError</span>: An error occurred while calling o1092.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 140.0 failed 1 times, most recent failure: Lost task 0.0 in stage 140.0 (TID 97, ip-10-172-253-6.us-west-2.compute.internal, executor driver): org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;29/2/2016 11:29&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:150)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:86)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:733)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:896)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:896)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:320)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.time.format.DateTimeParseException: Text &#39;29/2/2016 11:29&#39; could not be parsed at index 3\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2519)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2466)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2460)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1152)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1152)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2668)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2656)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2331)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2352)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2371)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2396)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1011)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:125)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:395)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1010)\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:304)\n\tat org.apache.spark.RangePartitioner.&lt;init&gt;(Partitioner.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:261)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:143)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:117)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:113)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:192)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\tat org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:192)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$3(SparkPlan.scala:247)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n\tat org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:87)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4(AdaptiveSparkPlanExec.scala:217)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$4$adapted(AdaptiveSparkPlanExec.scala:215)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:215)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:836)\n\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:201)\n\tat org.apache.spark.sql.execution.collect.Collector$.callExecuteCollect(Collector.scala:117)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:69)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:88)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:508)\n\tat org.apache.spark.sql.execution.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:480)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:396)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:2987)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3710)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2711)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3702)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$5(SQLExecution.scala:116)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:249)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:836)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:77)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:199)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3700)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2711)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2918)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:305)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:342)\n\tat sun.reflect.GeneratedMethodAccessor549.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:251)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: Fail to parse &#39;29/2/2016 11:29&#39; in the new parser. You can set spark.sql.legacy.timeParserPolicy to LEGACY to restore the behavior before Spark 3.0, or set to CORRECTED and treat it as an invalid datetime string.\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:150)\n\tat org.apache.spark.sql.catalyst.util.DateTimeFormatterHelper$$anonfun$checkParsedDiff$1.applyOrElse(DateTimeFormatterHelper.scala:141)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:86)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.parse(TimestampFormatter.scala:77)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:733)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:306)\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:304)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:896)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:896)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:60)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:356)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:320)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:144)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:117)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$9(Executor.scala:660)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1581)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:663)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.time.format.DateTimeParseException: Text &#39;29/2/2016 11:29&#39; could not be parsed at index 3\n\tat java.time.format.DateTimeFormatter.parseResolved0(DateTimeFormatter.java:1949)\n\tat java.time.format.DateTimeFormatter.parse(DateTimeFormatter.java:1777)\n\tat org.apache.spark.sql.catalyst.util.Iso8601TimestampFormatter.$anonfun$parse$1(TimestampFormatter.scala:78)\n\t... 25 more\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 3. People with maximum number of high impact incidents resolved"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"902352a1-d1ef-465c-b8e1-ccd97b5e4753"}}},{"cell_type":"code","source":["A3= df_unique_incidents.select([\"resolved_by\",\"impact\",\"duration\"]).\\\n    groupby([\"impact\",\"resolved_by\"]).count().\\\n    sort([\"impact\",\"count\"],ascending=[True,False])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffef8afc-0e86-4e60-8c59-41ee206bc8c8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["A3.show(n=10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d50e2cac-da2d-47ca-a4d0-973335faff19"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 4a. In each impact levels, find the person with most number of incidents resolved"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2fe5b98f-9f3f-41cc-8bfe-a9dc8cea6b3d"}}},{"cell_type":"code","source":["A4a=df_unique_incidents.select([\"resolved_by\",\"impact\",\"duration\"]).\\\n    groupby([\"impact\",\"resolved_by\"]).count().\\\n    sort([\"impact\",\"count\"],ascending=[True,False]).\\\n    dropDuplicates([\"impact\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9755b756-a45f-41db-87c9-1ff1d9d8a7b1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["A4a.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21984e2a-0d78-4f6a-b0f8-8f56ec2fc8af"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 4b. In each urgency levels, find the person with most number of incidents resolved"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8b3fc43-57bb-4070-9f1a-cf80e61541fa"}}},{"cell_type":"code","source":["A4b=df_unique_incidents.select([\"resolved_by\",\"urgency\",\"duration\"]).\\\n    groupby([\"urgency\",\"resolved_by\"]).count().\\\n    sort([\"urgency\",\"count\"],ascending=[True,False]).dropDuplicates([\"urgency\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e3048222-d527-481f-90cc-fcda14f15d96"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["A4b.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6a77f305-6bc9-4638-ab70-cde9646ddb40"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 4c. In each priority levels, find the person with most number of incidents resolved"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47ff5940-798a-4410-96ba-a44daee63606"}}},{"cell_type":"code","source":["A4c=df_unique_incidents.select([\"resolved_by\",\"priority\",\"duration\"]).\\\n    groupby([\"priority\",\"resolved_by\"]).count().\\\n    sort([\"priority\",\"count\"],ascending=[True,False]).dropDuplicates([\"priority\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52ef9694-bb73-4ddd-b1bb-c0ff210a06f9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["A4c.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf16dffe-dabe-40fe-b742-0c1c5e5d7813"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"ipynbError","data":"","errorSummary":"Command skipped","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 5. Find each contact type as a percentage of total incidents"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2387f425-e8d0-4433-a588-8bbf331ea4da"}}},{"cell_type":"code","source":["from pyspark.sql.window import Window"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2b90e526-ce57-4fc2-9bce-a0beb9178a25"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["A5= df_unique_incidents.select([\"contact_type\"]).\\\n    groupby([\"contact_type\"]).count().\\\n    withColumn(\"percentage\",F.round(F.col(\"count\")*100/F.sum(\"count\").over(Window.partitionBy()),2))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8df4d6c0-d3c5-4cd1-ad25-a320a8147953"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"A5","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"contact_type","nullable":true,"type":"string"},{"metadata":{},"name":"count","nullable":false,"type":"long"},{"metadata":{},"name":"percentage","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["A5.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"68075156-2bec-4a0d-a267-c210c257127c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+--------------+-----+----------+\n|  contact_type|count|percentage|\n+--------------+-----+----------+\n|         Phone|24688|     99.08|\n|         Email|   59|      0.24|\n|  Self service|  158|      0.63|\n|           IVR|    9|      0.04|\n|Direct opening|    4|      0.02|\n+--------------+-----+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------+-----+----------+\n  contact_type|count|percentage|\n+--------------+-----+----------+\n         Phone|24688|     99.08|\n         Email|   59|      0.24|\n  Self service|  158|      0.63|\n           IVR|    9|      0.04|\nDirect opening|    4|      0.02|\n+--------------+-----+----------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"281aa26e-fc48-4c63-9cd7-992b150581ba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### 6. On each priority level, find the percentage of incidents which made SLA and which did not."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40d0873f-b2eb-4bab-81f4-7c43de8df121"}}},{"cell_type":"code","source":["A6= df_unique_incidents.select([\"priority\",\"made_sla\"]).\\\n    groupby([\"priority\",\"made_sla\"]).count().\\\n    withColumnRenamed(\"count\",\"Population\").\\\n    withColumn(\"Made SLA %\",F.round(F.col(\"Population\")*100/F.sum(\"Population\").over(Window.partitionBy(\"priority\")),2)).\\\n    sort([\"priority\",\"made_sla\"],ascending=[True,False])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6ef0d79d-625c-438b-abd0-35cae7a5d440"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"A6","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"priority","nullable":true,"type":"string"},{"metadata":{},"name":"made_sla","nullable":true,"type":"boolean"},{"metadata":{},"name":"Population","nullable":false,"type":"long"},{"metadata":{},"name":"Made SLA %","nullable":true,"type":"double"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_unique_incidents.select([\"priority\",\"made_sla\"]).groupby([\"priority\",\"made_sla\"]).count().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a5312cf-bb47-4b02-a1d1-a6d7f15894c6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+--------+-----+\n|    priority|made_sla|count|\n+------------+--------+-----+\n|     4 - Low|    true|  651|\n|3 - Moderate|   false| 8321|\n|1 - Critical|    true|    5|\n|     4 - Low|   false|  123|\n|1 - Critical|   false|  265|\n|    2 - High|   false|  406|\n|3 - Moderate|    true|15145|\n|    2 - High|    true|    2|\n+------------+--------+-----+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+--------+-----+\n    priority|made_sla|count|\n+------------+--------+-----+\n     4 - Low|    true|  651|\n3 - Moderate|   false| 8321|\n1 - Critical|    true|    5|\n     4 - Low|   false|  123|\n1 - Critical|   false|  265|\n    2 - High|   false|  406|\n3 - Moderate|    true|15145|\n    2 - High|    true|    2|\n+------------+--------+-----+\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["A6.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1aaafb67-23d8-499a-b0f8-9c695e888391"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+--------+----------+----------+\n|    priority|made_sla|Population|Made SLA %|\n+------------+--------+----------+----------+\n|1 - Critical|    true|         5|      1.85|\n|1 - Critical|   false|       265|     98.15|\n|    2 - High|    true|         2|      0.49|\n|    2 - High|   false|       406|     99.51|\n|3 - Moderate|    true|     15145|     64.54|\n|3 - Moderate|   false|      8321|     35.46|\n|     4 - Low|    true|       651|     84.11|\n|     4 - Low|   false|       123|     15.89|\n+------------+--------+----------+----------+\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+--------+----------+----------+\n    priority|made_sla|Population|Made SLA %|\n+------------+--------+----------+----------+\n1 - Critical|    true|         5|      1.85|\n1 - Critical|   false|       265|     98.15|\n    2 - High|    true|         2|      0.49|\n    2 - High|   false|       406|     99.51|\n3 - Moderate|    true|     15145|     64.54|\n3 - Moderate|   false|      8321|     35.46|\n     4 - Low|    true|       651|     84.11|\n     4 - Low|   false|       123|     15.89|\n+------------+--------+----------+----------+\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 7. Top 5 location with the maximum number of incidents reported"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4285626c-459b-40da-a657-adb51c7539c6"}}},{"cell_type":"code","source":["A7= df_unique_incidents.groupby([\"location\"]).agg({\"number\":\"count\"}).\\\n    withColumnRenamed(\"count(number)\",\"Incidents Reported\").sort([\"Incidents Reported\"],ascending=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"11974c97-fd91-49c1-b4ac-38a3cba0df41"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"A7","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"location","nullable":true,"type":"string"},{"metadata":{},"name":"Incidents Reported","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_unique_incidents.groupby([\"location\"]).count().\\\nwithColumnRenamed(\"count\",\"Incident Reported\").sort([\"Incident Reported\"],ascending=False).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ede9ce64-6a7f-41a2-9587-fa3f5ffb1b0b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+-----------------+\n|    location|Incident Reported|\n+------------+-----------------+\n|Location 204|             5554|\n|Location 161|             4002|\n|Location 143|             3276|\n|Location 108|             2140|\n| Location 93|             1934|\n+------------+-----------------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+-----------------+\n    location|Incident Reported|\n+------------+-----------------+\nLocation 204|             5554|\nLocation 161|             4002|\nLocation 143|             3276|\nLocation 108|             2140|\n Location 93|             1934|\n+------------+-----------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["A7.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bef4e659-f099-4fff-bc48-9fbe4b5e3bd5"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+------------+------------------+\n|    location|Incidents Reported|\n+------------+------------------+\n|Location 204|              5554|\n|Location 161|              4002|\n|Location 143|              3276|\n|Location 108|              2140|\n| Location 93|              1934|\n+------------+------------------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+------------------+\n    location|Incidents Reported|\n+------------+------------------+\nLocation 204|              5554|\nLocation 161|              4002|\nLocation 143|              3276|\nLocation 108|              2140|\n Location 93|              1934|\n+------------+------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["#### 8. Which category of issues missed meeting the SLA the most?"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26102914-ea2b-44d1-a7e9-27515a0cc110"}}},{"cell_type":"code","source":["A8= df_unique_incidents.filter(\"made_sla==false\").groupby([\"category\"]).\\\n    agg({\"made_sla\":\"count\"}).withColumnRenamed(\"count(made_sla)\",\"Incidents failed to make SLA\").\\\n    sort([\"Incidents failed to make SLA\"],ascending=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0a824340-9d9c-4be4-b668-560c6abc5591"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"A8","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"category","nullable":true,"type":"string"},{"metadata":{},"name":"Incidents failed to make SLA","nullable":false,"type":"long"}],"type":"struct"},"tableIdentifier":null}],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["A8.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cf293346-5030-4332-ad32-224eda112ee4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------+----------------------------+\n|   category|Incidents failed to make SLA|\n+-----------+----------------------------+\n|Category 46|                        1254|\n|Category 26|                        1017|\n|Category 53|                        1009|\n|Category 42|                         689|\n|Category 23|                         505|\n+-----------+----------------------------+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----------------------------+\n   category|Incidents failed to make SLA|\n+-----------+----------------------------+\nCategory 46|                        1254|\nCategory 26|                        1017|\nCategory 53|                        1009|\nCategory 42|                         689|\nCategory 23|                         505|\n+-----------+----------------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["df_unique_incidents.filter(\"made_sla==false\").groupby([\"category\"]).count().sort(['count'],ascending=False).show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f24e8d05-3c8d-458f-a653-36a4e2e66032"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">+-----------+-----+\n|   category|count|\n+-----------+-----+\n|Category 46| 1254|\n|Category 26| 1017|\n|Category 53| 1009|\n|Category 42|  689|\n|Category 23|  505|\n+-----------+-----+\nonly showing top 5 rows\n\n</div>","removedWidgets":[],"addedWidgets":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+-----+\n   category|count|\n+-----------+-----+\nCategory 46| 1254|\nCategory 26| 1017|\nCategory 53| 1009|\nCategory 42|  689|\nCategory 23|  505|\n+-----------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"10a3edbe-4f6d-4919-ace6-d75e860f8098"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Incident_Management_Spark_Dataframe_Operations","dashboards":[],"language":"python","widgets":{},"notebookOrigID":699501117654341}},"nbformat":4,"nbformat_minor":0}
